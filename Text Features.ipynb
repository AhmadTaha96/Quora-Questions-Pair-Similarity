{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Features.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E0IkvdOsNB6L",
        "DqKm_mJtNaWk"
      ],
      "mount_file_id": "1zBVUkg49i5_TINUcDgsUHhuaUz4bajEt",
      "authorship_tag": "ABX9TyP7jWSpF3dWKZzq97qqnR55"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1j6Y20VMpge"
      },
      "source": [
        "Now after we have designed features related to status of questions in the dataset regardless of the actual text of the question itself, we shall design and create text-based features and then merge them with the original features in order to get the final features space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0IkvdOsNB6L"
      },
      "source": [
        "# Importing Lbraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4q-1FoAND3B"
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "import en_core_web_lg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqKm_mJtNaWk"
      },
      "source": [
        "# Text-Based Features Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y8JenlLfQ1z"
      },
      "source": [
        "Reading the original dataset as we will only use the original question1 and question2, no need for other features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49DOwYS4Nd9H"
      },
      "source": [
        "train_data = pd.read_csv(\"Data/train_data.csv\", compression = \"bz2\")\n",
        "test_data = pd.read_csv(\"Data/test_data.csv\", compression = \"bz2\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCCUMTkaOitn"
      },
      "source": [
        "questions1 = (train_data[\"question1\"]).apply(lambda x : str(x))\n",
        "\n",
        "questions2 = (train_data[\"question2\"]).apply(lambda x : str(x))\n",
        "\n",
        "questions = list(questions1 + \" \" + questions2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqy0EviCVTXg"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "train_feats = tfidf.fit_transform(questions)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7SW_qyLcKfe",
        "outputId": "ca057a7a-a4cd-4125-bf24-5869fee6ab22"
      },
      "source": [
        "print(\"Each sentence has represented using vector with length =\", train_feats.get_shape()[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Each sentence has represented using vector with length = 59480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdWEiakycYIv"
      },
      "source": [
        " words_in_train_data = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOfxiT_UdQQJ"
      },
      "source": [
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4aJcOZDVSBs"
      },
      "source": [
        "vectors = pd.DataFrame(columns = [np.arange(300)])\n",
        "\n",
        "i = 0\n",
        "\n",
        "for question in questions:\n",
        "\n",
        "  idf = 0\n",
        "  vector = np.zeros(300, dtype = np.float32)\n",
        "  \n",
        "  for word in question.split():\n",
        "\n",
        "     if word in words_in_train_data:\n",
        "       \n",
        "       word_idf = words_in_train_data[word]\n",
        "       idf += words_in_train_data[word]\n",
        "       vector = np.add(vector, nlp(word).vector * word_idf)\n",
        "     \n",
        "     else:\n",
        "       vector = np.add(vector, nlp(word).vector)\n",
        "\n",
        "  if idf != 0:\n",
        "    vector = np.divide(vector, idf)\n",
        "  \n",
        "  vectors.loc[i] = vector\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDz48sW-drpn"
      },
      "source": [
        "vectors.to_pickle(\"train_vectors.pkl\", compression = \"bz2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMQ6UfHIeBDm"
      },
      "source": [
        "questions1 = (test_data[\"question1\"]).apply(lambda x : str(x))\n",
        "\n",
        "questions2 = (test_data[\"question2\"]).apply(lambda x : str(x))\n",
        "\n",
        "questions = list(questions1 + \" \" + questions2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI4hRoOpeczJ"
      },
      "source": [
        "We here take the Idf of the word in each document in test data from train data transformed using TfIdf, we should not transformed test data, as test data here is just for testing but in real life it's not related to each other, so we can't count on it to do anyting but just use the train data as it is the source of our informations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGIZ4WvGdpoo"
      },
      "source": [
        "vectors = pd.DataFrame(columns = [np.arange(300)])\n",
        "\n",
        "i = 0\n",
        "\n",
        "for question in questions:\n",
        "\n",
        "  idf = 0\n",
        "  vector = np.zeros(300, dtype = np.float32)\n",
        "  \n",
        "  for word in question.split():\n",
        "\n",
        "     if word in words_in_train_data:\n",
        "       \n",
        "       word_idf = words_in_train_data[word]\n",
        "       idf += words_in_train_data[word]\n",
        "       vector = np.add(vector, nlp(word).vector * word_idf)\n",
        "     \n",
        "     else:\n",
        "       vector = np.add(vector, nlp(word).vector)\n",
        "\n",
        "  if idf != 0:\n",
        "    vector = np.divide(vector, idf)\n",
        "  \n",
        "  vectors.loc[i] = vector\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FanyIm0Ie-xE"
      },
      "source": [
        "vectors.to_pickle(\"test_vectors.pkl\", compression = \"bz2\")"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}